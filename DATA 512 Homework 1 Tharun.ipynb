{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"mount_file_id":"15NbfzOrU4LJnU3cIsP6-XIGGnADmyHxL","authorship_tag":"ABX9TyPRjUsDCL/XV3LhzTDiNoN6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Step 1: Data Acquisition\n","In order to measure article traffic from 2015-2022, you will need to collect data from the Pageviews API. The Pageviews API (documentation, endpoint) provides access to desktop, mobile web, and mobile app traffic data from July 2015 through the previous complete month.\n","\n","You will be collecting counts of pageviews using a specified subset of Wikipedia article pages. This is a subset of the English Wikipedia that represents a large number of dinosaur related articles.\n","\n","You will use the same article subset to generate several related data sets. All of the data sets are time series of monthly activity. For all of the data sets we are only interested in actual user pageview requests. The three resulting datasets should be saved as a JSON files ordered using article titles as a key for the resulting time series data. You should store the time series data as returned from the API, with the exception of removing the ‘access’ field as it is misleading for mobile and cumulative files."],"metadata":{"id":"5XGufenTi-IG"}},{"cell_type":"markdown","source":["## Step 1a - Configure the API request endpoints\n","\n","Provide the request header and parameters with dynamic values for access and article."],"metadata":{"id":"Gl3xNAlXl1dR"}},{"cell_type":"markdown","source":["### Import the required packages\n","\n","Importing the packages for extraction, transformation, plotting the data"],"metadata":{"id":"SZ-89krEmHse"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"z9fJRaaR6LZZ","executionInfo":{"status":"ok","timestamp":1664911951776,"user_tz":420,"elapsed":834,"user":{"displayName":"Tharun Kumar Reddy Karasani","userId":"13477090777886607117"}}},"outputs":[],"source":["# These are standard python modules\n","import json, time, urllib.parse\n","import pandas as pd\n","import numpy as np\n","import re\n","import requests\n","import matplotlib.pyplot as plt\n","from datetime import datetime"]},{"cell_type":"markdown","source":["### Configure the endpoint details\n","\n","All the endpoint details, headers and parameters are configured below. The titles of the articles are extracted from the dinosaur excel sheet provided which contains 1423 unique articles."],"metadata":{"id":"mPnUL-Z3mhlz"}},{"cell_type":"code","source":["# This is just a list of dinosaur Wikipedia article titles that we can use for example requests\n","dinosaurs_raw = pd.read_excel('/content/drive/MyDrive/MS Admission/Washington/GitHub/DataScienceProjects/data-512-homework_1/dinosaur_genera.cleaned.SEPT.2022.xlsx')\n","ARTICLE_TITLES = dinosaurs_raw['name'].to_list()\n","\n","# The REST API 'pageviews' URL - this is the common URL/endpoint for all 'pageviews' API requests\n","API_REQUEST_PAGEVIEWS_ENDPOINT = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/'\n","\n","# This is a parameterized string that specifies what kind of pageviews request we are going to make\n","# In this case it will be a 'per-article' based request. The string is a format string so that we can\n","# replace each parameter with an appropriate value before making the request\n","API_REQUEST_PER_ARTICLE_PARAMS = 'per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}'\n","\n","# The Pageviews API asks that we not exceed 100 requests per second, we add a small delay to each request\n","API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n","API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n","\n","# When making a request to the Wikimedia API they ask that you include a \"unique ID\" that will allow them to\n","# contact you if something happens - such as - your code exceeding request limits - or some other error happens\n","REQUEST_HEADERS = {\n","    'User-Agent': '<karasth@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2022',\n","}\n","\n","# REST API parameters for the json data blob from the website\n","ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE = {\n","    \"project\":     \"en.wikipedia.org\",\n","    \"access\":      \"desktop\",      # this should be changed for the different access types\n","    \"agent\":       \"user\",\n","    \"article\":     \"\",             # this value will be set/changed before each request\n","    \"granularity\": \"monthly\",\n","    \"start\":       \"2015070100\",   # this is the provided start date\n","    \"end\":         \"2022093000\"    # this is the provided end date\n","}"],"metadata":{"id":"DkZg83is7ItU","executionInfo":{"status":"ok","timestamp":1664911952073,"user_tz":420,"elapsed":302,"user":{"displayName":"Tharun Kumar Reddy Karasani","userId":"13477090777886607117"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### Function for REST API call to get the article related information\n","\n","Steps involved in the functione execution:\n","1. Pass the header, parameters and endpoint details for the GET request\n","2. Trigger the REST API GET request for each of the article and access type combination\n","3. Catch the exceptions where we couldn't extract the data from the endpoint"],"metadata":{"id":"CbOcATP-mq_4"}},{"cell_type":"code","source":["def request_pageviews_per_article(article_title = None, \n","                                  endpoint_url = API_REQUEST_PAGEVIEWS_ENDPOINT, \n","                                  endpoint_params = API_REQUEST_PER_ARTICLE_PARAMS, \n","                                  request_template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE,\n","                                  headers = REQUEST_HEADERS):\n","    # Make sure we have an article title\n","    if not article_title: return None\n","    \n","    # Titles are supposed to have spaces replaced with \"_\" and be URL encoded\n","    article_title_encoded = urllib.parse.quote(article_title.replace(' ','_'))\n","    request_template['article'] = article_title_encoded\n","    \n","    # now, create a request URL by combining the endpoint_url with the parameters for the request\n","    request_url = endpoint_url+endpoint_params.format(**request_template)\n","    \n","    # make the request\n","    try:\n","        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n","        # occurs during the request processing - throttling is always a good practice with a free\n","        # data source like Wikipedia - or other community sources\n","        if API_THROTTLE_WAIT > 0.0:\n","            time.sleep(API_THROTTLE_WAIT)\n","        response = requests.get(request_url, headers=headers)\n","        json_response = response.json()\n","    except Exception as e:\n","        print(e)\n","        json_response = None\n","    return json_response\n"],"metadata":{"id":"DAMtNUSg6Qre","executionInfo":{"status":"ok","timestamp":1664911952074,"user_tz":420,"elapsed":5,"user":{"displayName":"Tharun Kumar Reddy Karasani","userId":"13477090777886607117"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Step 1b - Extract the wikipedia information for the given dinosaur article names\n","\n","This step involves extracting the data for all the dinosaurs and all access types."],"metadata":{"id":"rD5QVnLZnf0Z"}},{"cell_type":"markdown","source":["### Extract the wikipedia article information for various access types (Desktop, Mobile, All)\n","\n","Added exception handling to highlight the articles for which we couldn't pull the information using the API endpoint"],"metadata":{"id":"rIyQKveVm4hR"}},{"cell_type":"code","source":["access_list = ['all-access', 'desktop', 'mobile-app', 'mobile-web']\n","df_access = list()\n","for i in ARTICLE_TITLES:\n","  # Exception handling to make sure code doesn't break\n","  try:\n","    for j in access_list:\n","      ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE['access'] = j\n","      views = request_pageviews_per_article(article_title = i, request_template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE)\n","      df_access.append(pd.json_normalize(views['items']))\n","    #print(\"Obtained the pageview data for: \",i)\n","  except:\n","    print(\"Couldn't get the pageview data for: \",i)\n","\n","# Obtain the overall dinosaurs data\n","df_dinosaurs = pd.concat(df_access)"],"metadata":{"id":"xbyHJ48w6Txr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Save the data as excel file for future reference (Temporary)\n","\n","This is a temporary step which can be neglected. "],"metadata":{"id":"bxoCfr5fnI4a"}},{"cell_type":"code","source":["# Write the raw data to excel\n","df_dinosaurs.to_excel('/content/drive/MyDrive/MS Admission/Washington/GitHub/DataScienceProjects/data-512-homework_1/dinosaurs_all.xlsx')\n","# Read the raw data from excel\n","df_dinosaurs = pd.read_excel('/content/drive/MyDrive/MS Admission/Washington/GitHub/DataScienceProjects/data-512-homework_1/dinosaurs_all.xlsx')\n","df_dinosaurs.info()"],"metadata":{"id":"IO3FtFm1R5Af"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 1c - Load the JSON data into respective files based on access type\n","\n","**Monthly mobile access - ** \n","The API separates mobile access types into two separate requests, you will need to sum these to make one count for all mobile pageviews. You should store the mobile access data in a file called:\n","dino_monthly_mobile_startYYYYMM-endYYYYMM.json\n","\n","**Monthly desktop access - ** Monthly desktop page traffic is based on one single request. You should store the desktop access data in a file called:\n","dino_monthly_desktop_startYYYYMM-endYYYYMM.json\n","\n","**Monthly cumulative - ** Monthly cumulative data is the sum of all mobile, and all desktop traffic per article. You should store the monthly cumulative data in a file called:\n","dino_monthly_cumulative_startYYYYMM-endYYYYMM.json\n","\n","For all of the files the startYYYYMM and endYYYYMM represent the starting and ending year and month as integer text.\n"],"metadata":{"id":"tIf8_jH9oE9h"}},{"cell_type":"markdown","source":["### Function to write the JSON data for each access type"],"metadata":{"id":"MnB9RfJpnSlg"}},{"cell_type":"code","source":["def write_json(data = df_dinosaurs, access_name = 'desktop'):\n","  # Aggregate the data for access type mobile\n","  if access_name == 'mobile':\n","    columns = data.columns.to_list()\n","    columns.remove('views')\n","    data = data.groupby(columns).agg({'views': np.sum}).reset_index()\n","  \n","  # Get the cumulative total\n","  if access_name == 'cumulative':\n","    data['views'] = data['views'].cumsum()\n","\n","  result = data.to_json(orient='records')[1:-1].replace('},{', '} {')\n","  result = '['+result+']'\n","  result = re.sub(\"}\\s{\", \"},{\", result)\n","  parsed = json.loads(result)\n","  json_object = json.dumps(parsed, indent=4)\n","  # Path to write JSON file\n","  path = f'''/content/drive/MyDrive/MS Admission/Washington/GitHub/DataScienceProjects/data-512-homework_1/json/dino_monthly_{access_name}_201507-202209.json'''\n","  with open(path, 'w') as f:\n","      f.write(json_object)\n","\n","  print(\"JSON write complete for access type: \", access_name)"],"metadata":{"id":"C9pzHbUxfW4f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Function to segregate the data into different files based on the access type\n","\n","Pass the data and segregate into different JSON files"],"metadata":{"id":"u-EhugCrn0U6"}},{"cell_type":"code","source":["def process_json(data):\n","  data.reset_index(inplace=True)\n","  # Desktop access type\n","  write_json(data[data['access']=='desktop'].drop(['access', 'Unnamed: 0', 'index'], axis = 1), 'desktop')\n","  # Mobile access type\n","  write_json(data[(data['access']=='mobile-app') | (data['access'] == 'mobile-web')].drop(['access', 'Unnamed: 0', 'index'], axis = 1), 'mobile')\n","  # Cumulative (all-access type)\n","  write_json(data[data['access']=='all-access'].drop(['access', 'Unnamed: 0', 'index'], axis = 1), 'cumulative')"],"metadata":{"id":"dYDFlnLZVH9f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Write the data to the respective JSON files\n","process_json(df_dinosaurs)"],"metadata":{"id":"Mh39bmBTbl-E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 2 : Analysis\n","You will conduct a very basic visual analysis. The analysis for this homework is to graph specific subsets of the data as a timeseries. You will produce three different graphs."],"metadata":{"id":"T3AEUhmFgscL"}},{"cell_type":"markdown","source":["## Step 2a - Maximum Average and Minimum Average \n","The first graph should contain time series for the articles that have the highest average page requests and the lowest average page requests for desktop access and mobile access. Your graph should have four lines (max desktop, min desktop, max mobile, min mobile)."],"metadata":{"id":"RdC9FzxhpO05"}},{"cell_type":"markdown","source":["### Read the JSON files obtained from Step 1\n","\n","Read the respective JSON files for mobile and desktop access types and load the data into dataframes for further transformation"],"metadata":{"id":"l_gj-QlGrJTA"}},{"cell_type":"code","source":["# Read the input json files\n","desktop = open('/content/drive/MyDrive/MS Admission/Washington/GitHub/DataScienceProjects/data-512-homework_1/json/dino_monthly_desktop_201507-202209.json')\n","mobile = open('/content/drive/MyDrive/MS Admission/Washington/GitHub/DataScienceProjects/data-512-homework_1/json/dino_monthly_mobile_201507-202209.json')\n","\n","# Load the JSON fiels saved in step 1\n","json_desktop = json.load(desktop)\n","json_mobile = json.load(mobile)\n","\n","# Convert the JSON data into dataframes for further transformation\n","df_desktop = pd.json_normalize(json_desktop)\n","df_mobile = pd.json_normalize(json_mobile)\n","df_desktop.head()\n","df_mobile.head()"],"metadata":{"id":"Uh7MmLtOhqhw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Function to get the data for articles that have the highest and lowest average page requests over all the months\n","\n","Steps followed by the function for execution:\n","\n","1. Calculate the average of article views over all the available months\n","2. Get the article names with maximum (highest) and minimum (lowest) average views\n","3. Obtain and return the wikipedia data for the above articles with category label"],"metadata":{"id":"Zx6yxv2tsCFh"}},{"cell_type":"code","source":["def max_min_avg(data = None, access = 'desktop'):\n","  grouped = data.groupby(\"article\").mean('views')\n","  data['timestamp'] = data['timestamp'].astype('str')\n","  data['timestamp'] = pd.to_datetime(data['timestamp'], format='%Y%m%d%H')\n","  # Obtain the articles with maximum and minimum average page requests\n","  max_name = grouped.loc[grouped['views'].idxmax()].name\n","  min_name = grouped.loc[grouped['views'].idxmin()].name\n","  #print(max_name, min_name)\n","  # Filter the data for the above articles\n","  data_max = data[data['article'] == max_name][['article', 'timestamp', 'views']]\n","  data_min = data[data['article'] == min_name][['article', 'timestamp', 'views']]\n","  # Obtain the label for time series plot\n","  data_max['category'] = 'max_' + access\n","  data_max['label'] = data_max['category'] + '[' + data_max['article'] + ']'\n","  data_min['category'] = 'min_' + access\n","  data_min['label'] = data_min['category'] + '[' + data_min['article'] + ']'\n","  data = data_max.append(data_min)\n","  return data"],"metadata":{"id":"2p_E8BdajgGe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Obtain the data for both desktop and mobile access type\n","\n","Pass both the access type labels to the above function. The data returned is then appended to a main dataframe which is used for plotting"],"metadata":{"id":"rZFH-ZndseVZ"}},{"cell_type":"code","source":["# Obtain the data for time series plots\n","desktop = max_min_avg(df_desktop, access = 'desktop')\n","mobile = max_min_avg(df_mobile, access = 'mobile')\n","# Combine both the access type datasets\n","df_main = desktop.append(mobile)\n","df_main = df_main.set_index('timestamp')"],"metadata":{"id":"nmujWUTejyDH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Plot the data for the articles with highest and lowest average page views for both desktop and mobile\n","\n","A total of four time series plots will be generated (two for each access type). All four time series plots are overlapped into a single plot for comparison"],"metadata":{"id":"WsXxDJs0sF_7"}},{"cell_type":"code","source":["def plot_maxmin(data):\n","  # Declare the figure size\n","  plt.figure(figsize=(16, 8), dpi=150)\n","  # Plot the data values\n","  df1 = df_main[df_main['category'] == 'max_desktop']\n","  df1['views'].plot(label=df1['label'].unique()[0], color='orange')\n","  df2 = df_main[df_main['category'] == 'min_desktop']\n","  df2['views'].plot(label=df2['label'].unique()[0], color='blue')\n","  df3 = df_main[df_main['category'] == 'max_mobile']\n","  df3['views'].plot(label=df3['label'].unique()[0], color='green')\n","  df4 = df_main[df_main['category'] == 'min_mobile']\n","  df4['views'].plot(label=df4['label'].unique()[0], color='black')\n","\n","  # adding title to the plot\n","  plt.title('Maximum average and Minimum Average')\n","  # adding Label to the x-axis and y-axis\n","  plt.xlabel('Month of Visit')\n","  plt.ylabel('Views')\n","  # adding legend to the curve\n","  plt.legend()\n","\n","  # Save the plot as a png file\n","  plt.savefig('/content/drive/MyDrive/MS Admission/Washington/GitHub/DataScienceProjects/data-512-homework_1/plots/max_min_average_views.png')"],"metadata":{"id":"8derw6d8m5zO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generate the Plot"],"metadata":{"id":"N9AZht_1wehT"}},{"cell_type":"code","source":["plot_maxmin(df_main)"],"metadata":{"id":"wWqQaHKUlO02"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 2b - Top 10 Peak Page Views\n","The second graph should contain time series for the top 10 article pages by largest (peak) page views over the entire time by access type. You first find the month for each article that contains the highest (peak) page views, and then order the articles by these peak values. Your graph should contain the top 10 for desktop and top 10 for mobile access (20 lines)."],"metadata":{"id":"nq4cgik5IJSI"}},{"cell_type":"markdown","source":["### Function to get the articles list that have the top 10 page views across all the months\n","\n","Steps followed by the function for execution:\n","\n","1. Calculate the maximum views across all the months for each article\n","2. Get the top 10 articles that have the largest page views and assign them a rank based on the peak views value\n","3. Obtain and return the wikipedia data for the above top 10 articles with category label"],"metadata":{"id":"ZWpMNxJwqgAR"}},{"cell_type":"code","source":["def top_ten(data = None, access = 'desktop'):\n","  # Obtain the top 10 articles with largest page views\n","  grouped = data.groupby(\"article\").max('views').reset_index()\n","  grouped = grouped.sort_values(by='views', ascending=False)\n","  # Get the top 10 articles\n","  top10 = grouped.head(10)\n","  top10 = top10[['article']].reset_index()\n","  top10 = top10.drop('index', axis = 1)\n","  top10['rank'] = pd.Series(np.arange(1,11))\n","  data['access'] = access\n","  data = data[['article', 'timestamp', 'views', 'access']]\n","  # Filter the data for the above articles\n","  data = data.merge(top10, on='article', how = 'inner', suffixes=('_1', '_2'))\n","  # Obtain the label for time series plot\n","  data['category'] = access + '_top_' + data['rank'].astype('str') + '(' + data['article'] +')'\n","  return data"],"metadata":{"id":"JRuDeUoy8N85"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Obtain the data for both desktop and mobile access type\n","\n","Pass both the access type labels to the above function. The data returned is then appended to a main dataframe which is used for plotting"],"metadata":{"id":"A7MQb_MQqaYZ"}},{"cell_type":"code","source":["# Obtain the data for time series plots\n","desktop = top_ten(df_desktop, 'desktop')\n","mobile = top_ten(df_mobile, 'mobile')\n","# Combine both the access type datasets\n","df_main = desktop.append(mobile)\n","df_main = df_main.set_index('timestamp')"],"metadata":{"id":"NrqxyJlxKne1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Plot the data for the top 10 articles with highest page views for both desktop and mobile\n","\n","A total of 20 time series plots will be generated (ten for each access type). All 20 time series plots are overlapped into a single plot for comparison"],"metadata":{"id":"nlDLANc0qRI9"}},{"cell_type":"code","source":["def plot_topten(data):\n","  # Declare the figure size\n","  plt.figure(figsize=(16, 8), dpi=150)\n","  \n","  # Plot the data values\n","  for i in range(1,10):\n","    df = data[data['rank'] == i]\n","    df[df['access'] == 'desktop']['views'].plot(label=df['category'].unique()[0])\n","    df[df['access'] == 'mobile']['views'].plot(label=df['category'].unique()[1])\n","\n","  # adding title to the plot\n","  plt.title('Top 10 Peak Page Views')\n","  # adding Label to the x-axis and y-axis\n","  plt.xlabel('Month of Visit')\n","  plt.ylabel('Views')\n","  # adding legend to the curve\n","  plt.legend()\n","\n","  # Save the plot as a png file\n","  plt.savefig('/content/drive/MyDrive/MS Admission/Washington/GitHub/DataScienceProjects/data-512-homework_1/plots/top_10_peak_page_views.png')"],"metadata":{"id":"c9nNtxaYV0FW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generate the Plot"],"metadata":{"id":"NKnvlK7Swc8Q"}},{"cell_type":"code","source":["plot_topten(df_main)"],"metadata":{"id":"Y70Yk75LXnWp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 2c - Fewest Months of Data\n","The third graph should show pages that have the fewest months of available data. These will all be relatively short time series, some may only have one month of data. Your graph should show the 10 articles with the fewest months of data for desktop access and the 10 articles with the fewest months of data for mobile access.\n","\n","In order to complete the analysis correctly and receive full credit, your graph will need to be the right scale to view the data; all units, axes, and values should be clearly labeled. Your graph should possess a legend and a title. You must generate a .png or .jpeg formatted image of your final graph.\n","You should graph the data using tools or libraries within your notebook, rather than using an external application to facilitate reproducibility.\n"],"metadata":{"id":"H4M-HKvSISNq"}},{"cell_type":"markdown","source":["### Function to get the articles list that have the fewest months of available data\n","\n","Steps followed by the function for execution:\n","\n","1. Calculate the count of months with data available for each article\n","2. Get the top 10 articles that have the fewest months of data and assign them a rank based on the count of months with data availability\n","3. Obtain and return the wikipedia data for the above top 10 articles with category label"],"metadata":{"id":"nnazynvbpjc9"}},{"cell_type":"code","source":["def lessfreq_ten(data = None, access = 'desktop'):\n","  # Obtain the 10 articles with fewest months of data available\n","  grouped = data.groupby(\"article\").count().reset_index().iloc[:,0:2]\n","  grouped = grouped.rename(columns={\"project\": \"count\"})\n","  grouped = grouped.sort_values(by='count', ascending=True)\n","  # Obtain the top 10 articles\n","  top10 = grouped.head(10)\n","  top10 = top10[['article']].reset_index()\n","  top10 = top10.drop('index', axis = 1)\n","  top10['rank'] = pd.Series(np.arange(1,11))\n","  data['access'] = access\n","  data = data[['article', 'timestamp', 'views', 'access']]\n","  # Filter the data for the above articles\n","  data = data.merge(top10, on='article', how = 'inner', suffixes=('_1', '_2'))\n","  # Obtain the label for time series plot\n","  data['category'] = access + '_least_freq_' + data['rank'].astype('str') + '(' + data['article'] +')'\n","  return data"],"metadata":{"id":"C-QTtkXWITZw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Obtain the data for both desktop and mobile access type\n","\n","Pass both the access type labels to the above function. The data returned is then appended to a main dataframe which is used for plotting"],"metadata":{"id":"_Q91DRtfpvTk"}},{"cell_type":"code","source":["# Obtain the data for time series plots\n","desktop = lessfreq_ten(df_desktop, 'desktop')\n","mobile = lessfreq_ten(df_mobile, 'mobile')\n","# Combine both the access type datasets\n","df_main = desktop.append(mobile)\n","df_main = df_main.set_index('timestamp')"],"metadata":{"id":"MaSfx_Jpc7E-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Plot the data for the articles with fewest months of available data\n","\n","A total of 20 time series plots will be generated (ten for each access type). All 20 time series plots are overlapped into a single plot for comparison.\n","\n","Note: The data doesn't look complete for all the articles as few articles have the least data avilability."],"metadata":{"id":"g1CM4I1Vp1Ga"}},{"cell_type":"code","source":["def plot_freqten(data):\n","  # Declare the figure size\n","  plt.figure(figsize=(16, 8), dpi=150)\n","\n","  # Plot the data values\n","  for i in range(1,10):\n","    df = data[data['rank'] == i]\n","    df[df['access'] == 'desktop']['views'].plot(label=df['category'].unique()[0])\n","    df[df['access'] == 'mobile']['views'].plot(label=df['category'].unique()[1])\n","\n","  # adding title to the plot\n","  plt.title('Articles with Fewest Months of Data')\n","  # adding Label to the x-axis and y-axis\n","  plt.xlabel('Month of Visit')\n","  plt.ylabel('Views')\n","  # adding legend to the curve\n","  plt.legend()\n","\n","  # Save the plot as a png file\n","  plt.savefig('/content/drive/MyDrive/MS Admission/Washington/GitHub/DataScienceProjects/data-512-homework_1/plots/articles_with_fewest_months_data.png')"],"metadata":{"id":"PhpyCk45c9dg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generate the Plot"],"metadata":{"id":"jho646hxwZNt"}},{"cell_type":"code","source":["plot_freqten(df_main)"],"metadata":{"id":"f4JDbQB5c_Io"},"execution_count":null,"outputs":[]}]}